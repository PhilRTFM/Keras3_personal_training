inp <- keras3::layer_input(shape = list(base::as.integer(input_dim)))
x <- inp
# 1ère couche
x <- keras3::layer_dense(x, units = base::as.integer(hidden_units[1]), activation = activation)
# couches suivantes si présentes
if (length(hidden_units) > 1) {
for (u in hidden_units[-1]) {
x <- keras3::layer_dense(x, units = base::as.integer(u), activation = activation)
}
}
out <- keras3::layer_dense(x, units = base::as.integer(num_classes), activation = "softmax")
model <- keras3::keras_model(inp, out)
keras3::compile(
object = model,
optimizer = keras3::optimizer_adam(learning_rate = lr),
loss = "sparse_categorical_crossentropy",
metrics = list("sparse_categorical_accuracy")
)
return(model)
}
#' Prédit les étiquettes (factor) à partir d'un modèle MLP
#'
#' @param model keras3::keras_model Modèle MLP entraîné
#' @param x matrix|array Données de prédiction (n_obs x input_dim)
#' @param class_levels character Niveaux de classes (longueur K) ordre du softmax
#' @param verbose logical Messages (FALSE)
#' @return factor Vecteur factor des prédictions, niveaux = class_levels
mlp_predict_labels <- function(model, x, class_levels, verbose = FALSE) {
stopifnot(is.matrix(x) || is.array(x))
stopifnot(is.character(class_levels), length(class_levels) >= 2)
# Invocation via pipe / appel direct
probs <- model(x, verbose = base::as.integer(isTRUE(verbose)))
if (!is.matrix(probs)) probs <- base::as.matrix(probs)
idx <- base::max.col(probs, ties.method = "first")  # indices 1..K
pred <- base::factor(class_levels[idx], levels = class_levels)
return(pred)
}
# -- Non-supervisé : Autoencodeur (reconstruction X -> X') ---------------------
#' Construit un autoencodeur symétrique et expose encoder/decoder
#'
#' @param input_dim integer Dimension d'entrée (> 0)
#' @param latent_dim integer Taille de l'espace latent (>= 1)
#' @param units integer Vector de tailles des couches cachées de l'encodeur
#' @param activation character Activation des couches cachées (ex. "relu")
#' @param latent_activation character Activation de la couche latente (souvent "linear")
#' @param dropout numeric Taux de dropout (0 = inactif)
#' @param batchnorm logical Batch-normalization après chaque couche cachée
#' @param l2 numeric Coefficient de pénalisation L2 (0 = inactif)
#' @param lr numeric Learning rate Adam
#' @param verbose logical Messages (TRUE)
#' @return list Liste : autoencoder (keras_model), encoder (keras_model), decoder (keras_model)
#' @details
#' Modèle reconstruit X -> X' avec loss = MSE et métrique MAE. Décodeur symétrique.
ae_build <- function(input_dim,
latent_dim = 2L,
units = c(16L, 8L),
activation = "relu",
latent_activation = "linear",
dropout = 0,
batchnorm = FALSE,
l2 = 0,
lr = 1e-3,
verbose = TRUE) {
stopifnot(input_dim >= 1, latent_dim >= 1, length(units) >= 1)
if (isTRUE(verbose)) base::message("[ae_build] input_dim=", input_dim,
", latent_dim=", latent_dim,
", units=", base::paste(units, collapse = ","))
reg <- if (l2 > 0) keras3::regularizer_l2(l2) else NULL
# --- Encodeur ---
inp <- keras3::layer_input(shape = list(base::as.integer(input_dim)))
x <- inp
for (u in units) {
x <- keras3::layer_dense(x, units = base::as.integer(u), activation = activation, kernel_regularizer = reg)
if (isTRUE(batchnorm)) x <- keras3::layer_batch_normalization(x)
if (dropout > 0) x <- keras3::layer_dropout(x, rate = dropout)
}
z <- keras3::layer_dense(x, units = base::as.integer(latent_dim), activation = latent_activation, name = "latent")
# --- Décodeur (symétrique) ---
y <- z
for (u in rev(units)) {
y <- keras3::layer_dense(y, units = base::as.integer(u), activation = activation, kernel_regularizer = reg)
if (isTRUE(batchnorm)) y <- keras3::layer_batch_normalization(y)
if (dropout > 0) y <- keras3::layer_dropout(y, rate = dropout)
}
out <- keras3::layer_dense(y, units = base::as.integer(input_dim), activation = "linear", name = "recon")
ae  <- keras3::keras_model(inp, out, name = "autoencoder")
enc <- keras3::keras_model(inp, z,   name = "encoder")
# --- Décodeur autonome (Z -> X') ---
latent_inp <- keras3::layer_input(shape = list(base::as.integer(latent_dim)))
yd <- latent_inp
for (u in rev(units)) {
yd <- keras3::layer_dense(yd, units = base::as.integer(u), activation = activation, kernel_regularizer = reg)
}
dec_out <- keras3::layer_dense(yd, units = base::as.integer(input_dim), activation = "linear")
dec <- keras3::keras_model(latent_inp, dec_out, name = "decoder")
keras3::compile(
object = ae,
optimizer = keras3::optimizer_adam(learning_rate = lr),
loss = "mse",
metrics = list("mae")
)
res <- base::list(autoencoder = ae, encoder = enc, decoder = dec)
return(res)
}
# -- R/analysis.R : PCA | MDS ----
# Conventions : roxygen, pkg::fun(), verbose en dernier (par défaut FALSE), return() partout
# -- PCA avec FactoMineR et ggplot2 ----
#' PCA : deux plots côte à côte (individus + cercle pur)
#'
#'  install.packages("patchwork")
#'
#' @param x data.frame|matrix, données numériques
#' @param dims integer, dimensions à tracer (longueur 2)
#' @param group_factor factor|NULL, couleur des individus
#' @param scale_unit logical, standardiser (TRUE recommandé)
#' @param ellipse logical, ellipses de groupes (FALSE)
#' @param ellipse_level numeric, niveau des ellipses (0.95)
#' @param verbose logical, messages (FALSE)
#'
#' @return patchwork, deux plots côte à côte
pca_plot <- function(x, dims = c(1, 2), group_factor = NULL,
scale_unit = TRUE, ellipse = FALSE, ellipse_level = 0.95,
verbose = FALSE) {
if (isTRUE(verbose)) base::message("[PCA] Calcul de l'ACP…")
pca <- FactoMineR::PCA(base::as.data.frame(x), scale.unit = scale_unit, graph = FALSE)
# Individus
ind_coords <- pca$ind$coord[, dims, drop = FALSE]
df_ind <- tibble::tibble(
label = base::rownames(ind_coords),
Dim.1 = ind_coords[, 1],
Dim.2 = ind_coords[, 2]
)
if (!is.null(group_factor)) df_ind$group <- as.factor(group_factor)
p_ind <- ggplot2::ggplot(df_ind, ggplot2::aes(x = Dim.1, y = Dim.2, color = group)) +
ggplot2::geom_point() +
ggplot2::geom_text(ggplot2::aes(label = label), hjust = 0, vjust = 1.2, size = 3, show.legend = FALSE) +
ggplot2::geom_hline(yintercept = 0, linetype = "dashed") +
ggplot2::geom_vline(xintercept = 0, linetype = "dashed") +
ggplot2::labs(title = "PCA — individus", x = paste0("Dim ", dims[1]), y = paste0("Dim ", dims[2])) +
ggplot2::theme_minimal(base_size = 12) +
ggplot2::coord_equal()
if (ellipse && "group" %in% colnames(df_ind)) {
p_ind <- p_ind + ggplot2::stat_ellipse(ggplot2::aes(group = group), level = ellipse_level)
}
# Variables
var_coords <- pca$var$coord[, dims, drop = FALSE]
df_var <- tibble::tibble(
var = rownames(var_coords),
Dim.1 = var_coords[, 1],
Dim.2 = var_coords[, 2]
)
circle <- tibble::tibble(
t = seq(0, 2 * pi, length.out = 200),
x = cos(t),
y = sin(t)
)
p_var <- ggplot2::ggplot() +
ggplot2::geom_path(data = circle, ggplot2::aes(x = x, y = y)) +
ggplot2::geom_segment(data = df_var, ggplot2::aes(x = 0, y = 0, xend = Dim.1, yend = Dim.2),
arrow = ggplot2::arrow(length = ggplot2::unit(0.02, "npc"))) +
ggplot2::geom_text(data = df_var, ggplot2::aes(x = Dim.1, y = Dim.2, label = var),
hjust = 0, vjust = -0.5, size = 3) +
ggplot2::geom_hline(yintercept = 0, linetype = "dashed") +
ggplot2::geom_vline(xintercept = 0, linetype = "dashed") +
ggplot2::labs(title = "PCA — cercle des corrélations", x = paste0("Dim ", dims[1]), y = paste0("Dim ", dims[2])) +
ggplot2::coord_fixed(xlim = c(-1.1, 1.1), ylim = c(-1.1, 1.1)) +
ggplot2::theme_minimal(base_size = 12)
# Combine via patchwork
p_combined <- p_ind + p_var + patchwork::plot_layout(widths = c(1, 1))
if (isTRUE(verbose)) base::message("[PCA] Plot généré")
return(p_combined)
}
# pca_plot(iris[, 1:4], group_factor = iris$Species)
# -- MDS avec stats::cmdscale et MASS::isoMDS ----
#' Analyse MDS complète avec plots (projection + Shepard), couleurs par groupe
#'
#' @param x matrix|data.frame, données numériques (lignes = objets)
#' @param method character, méthode de distance ("euclidean", etc.)
#' @param metric logical, MDS métrique (TRUE) ou non-métrique (FALSE)
#' @param k integer, nombre de dimensions projetées (2)
#' @param labels character|NULL, étiquettes des points (sinon rownames)
#' @param group_factor factor|NULL, groupe pour coloration des points
#' @param title character|NULL, titre principal (NULL = automatique)
#' @param verbose logical, afficher les messages (FALSE)
#'
#' @return patchwork, 2 graphiques : MDS + Shepard
mds_plot <- function(x, method = "euclidean", metric = TRUE, k = 2,
labels = NULL, group_factor = NULL, title = NULL,
verbose = FALSE) {
if (isTRUE(verbose)) message("[MDS] Calcul des distances : ", method)
dist_obj <- stats::dist(x = x, method = method)
if (isTRUE(verbose)) message("[MDS] Exécution ", if (metric) "métrique" else "non-métrique")
if (metric) {
fit <- stats::cmdscale(d = dist_obj, k = k, eig = TRUE)
coords <- tibble::as_tibble(fit$points, .name_repair = "minimal")
} else {
fit <- MASS::isoMDS(d = dist_obj, k = k)
coords <- tibble::as_tibble(fit$points, .name_repair = "minimal")
}
colnames(coords) <- paste0("Dim.", seq_len(ncol(coords)))
if (is.null(labels)) labels <- rownames(x)
coords$label <- labels
if (!is.null(group_factor)) coords$group <- as.factor(group_factor)
title_a <- if (is.null(title)) "A. MDS — configuration des objets" else paste("A. ", title)
if (!is.null(group_factor)) {
p1 <- ggplot2::ggplot(coords, ggplot2::aes(x = .data$Dim.1, y = .data$Dim.2, color = .data$group)) +
ggplot2::geom_point() +
ggplot2::geom_text(ggplot2::aes(label = label), hjust = 0, vjust = -0.5, size = 3, na.rm = TRUE, show.legend = FALSE) +
ggplot2::labs(title = title_a, x = "Dim 1", y = "Dim 2", color = "Groupe")
} else {
p1 <- ggplot2::ggplot(coords, ggplot2::aes(x = .data$Dim.1, y = .data$Dim.2)) +
ggplot2::geom_point() +
ggplot2::geom_text(ggplot2::aes(label = label), hjust = 0, vjust = -0.5, size = 3, na.rm = TRUE) +
ggplot2::labs(title = title_a, x = "Dim 1", y = "Dim 2")
}
p1 <- p1 + ggplot2::theme_minimal(base_size = 12)
d_orig <- as.numeric(dist_obj)
d_proj <- as.numeric(stats::dist(coords[, startsWith(names(coords), "Dim."), drop = FALSE]))
df_shepard <- tibble::tibble(orig = d_orig, proj = d_proj)
rmse <- sqrt(mean((df_shepard$proj - df_shepard$orig)^2))
r <- cor(df_shepard$orig, df_shepard$proj)
title_b <- sprintf("B. Diagramme de Shepard (RMSE=%.3f, r=%.3f)", rmse, r)
p2 <- ggplot2::ggplot(df_shepard, ggplot2::aes(x = .data$orig, y = .data$proj)) +
ggplot2::geom_point(shape = 3, alpha = 0.6, size = 0.5) +
ggplot2::geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red", size = 0.5) +
ggplot2::geom_smooth(method = "lm", formula = y ~ x, se = FALSE, color = "blue") +
ggpmisc::stat_poly_eq(
aes(label = stat(eq.label)),
formula = y ~ x,
parse = TRUE,
color = "blue",
size = 3
) +
ggplot2::labs(title = title_b, x = "Distance originale", y = "Distance projetée") +
ggplot2::theme_minimal(base_size = 12)
if (isTRUE(verbose)) message("[MDS] Graphiques générés")
res <- p1 + p2 + patchwork::plot_layout(ncol = 2)
return(res)
}
# mds_plot(iris[, 1:4], labels = rownames(iris), group_factor = iris$Species)
# -- T-SNE ?! ----
# -- UMAP ?! ----
pca_plot(iris[, 1:4], group_factor = iris$Species)
mds_plot(iris[, 1:4], labels = rownames(iris), group_factor = iris$Species)
# -- R/metrics.R — Matrice de confusion + métriques & courbes d'entraînement ----
# Conventions : Roxygen, tout namespacé (pkg::fun), PAS de library()/install.*
# verbose en DERNIER (FALSE), toutes les fonctions se terminent par return(...)
# ============================================================================
#  Fonctions CONSERVÉES (à l’identique)
# ============================================================================
#' Heatmap double gradient (centre bleu, bords vert) sans ggnewscale
#'
#' @param df_long tibble Sortie de cm_with_sums_long()
#' @param title character Titre
#' @param y_lab character Libellé axe Y
#' @param core_pal character(2) Palette centre (hex)
#' @param sum_pal character(2) Palette sums (hex)
#' @param verbose logical Messages (FALSE)
#' @return ggplot2::ggplot Graphique heatmap
viz_confusion_heatmap_sums <- function(df_long,
title = "Matrice de confusion",
y_lab  = "prédiction du modèle",
core_pal = c("#cfe5f2", "#2b8cbe"),
sum_pal  = c("#a1d99b", "#31a354"),
verbose = FALSE) {
core_df <- dplyr::filter(df_long, !.data$is_sum)
sum_df  <- dplyr::filter(df_long,  .data$is_sum)
# Générateurs de couleurs
col_fun_core <- scales::col_numeric(core_pal, domain = c(0, 1), na.color = "#ffffff")
col_fun_sum  <- scales::col_numeric(sum_pal,  domain = c(0, 1), na.color = "#ffffff")
# Normalisation locale (par groupe)
norm01 <- function(x) {
rng <- base::range(x, na.rm = TRUE)
if (!base::is.finite(rng[1]) || !base::is.finite(rng[2]) || rng[1] == rng[2]) {
return(base::ifelse(base::is.na(x), NA_real_, 1))
}
return(scales::rescale(x, to = c(0, 1), from = rng))
}
if (nrow(core_df) > 0) core_df$fill_hex <- base::ifelse(base::is.na(core_df$Freq), "#ffffff", col_fun_core(norm01(core_df$Freq)))
if (nrow(sum_df)  > 0) sum_df$fill_hex  <- base::ifelse(base::is.na(sum_df$Freq),  "#ffffff", col_fun_sum(norm01(sum_df$Freq)))
p <- ggplot2::ggplot() +
ggplot2::geom_tile(data = core_df, ggplot2::aes(x = .data$Pred, y = .data$Truth, fill = .data$fill_hex), color = "white", linewidth = 0.6) +
ggplot2::geom_text(data = core_df, ggplot2::aes(x = .data$Pred, y = .data$Truth, label = base::ifelse(base::is.na(.data$Freq), "", .data$Freq)), size = 4.2) +
ggplot2::scale_fill_identity() +
ggplot2::geom_tile(data = sum_df,  ggplot2::aes(x = .data$Pred, y = .data$Truth, fill = .data$fill_hex), color = "white", linewidth = 0.6) +
ggplot2::geom_text(data = sum_df,  ggplot2::aes(x = .data$Pred, y = .data$Truth, label = base::ifelse(base::is.na(.data$Freq), "", .data$Freq)), size = 4.2) +
ggplot2::scale_x_discrete(position = "top") +
ggplot2::coord_fixed() +
ggplot2::labs(title = title, x = NULL, y = y_lab) +
ggplot2::theme_minimal(base_size = 12) +
ggplot2::theme(legend.position = "none", panel.grid = ggplot2::element_blank(), axis.title.x = ggplot2::element_blank(), axis.text.x = ggplot2::element_text(vjust = 0))
if (isTRUE(verbose)) base::message("[cm] heatmap (double gradient) prête.")
return(p)
}
#' Petit tableau ggplot des métriques globales (macro)
#'
#' @param metrics_list list Sortie de metrics_build_from_cm() ou de metrics_from_cm()
#' @param digits integer Nombre de décimales
#' @param verbose logical Messages (FALSE)
#' @return ggplot2::ggplot Tableau sous forme de ggplot (via ggplotify)
metrics_table_plot <- function(metrics_list, digits = 2, verbose = FALSE) {
# Rendre compatible avec deux schémas de noms possibles
macro <- if (!base::is.null(metrics_list$macro_tbl)) metrics_list$macro_tbl else metrics_list$macro
acc   <- metrics_list$accuracy
if (base::is.null(macro) || base::is.null(acc)) base::stop("metrics_list doit contenir 'accuracy' et macro(_tbl).")
# Colonnes attendues (avec fallback sur différents noms)
spec <- if (!base::is.null(macro$specificity)) macro$specificity else if (!base::is.null(macro$Specificity)) macro$Specificity else NA_real_
rec  <- if (!base::is.null(macro$macro_recall)) macro$macro_recall else if (!base::is.null(macro$Recall)) macro$Recall else NA_real_
f1   <- if (!base::is.null(macro$macro_f1)) macro$macro_f1 else if (!base::is.null(macro$F1)) macro$F1 else NA_real_
df_wide <- tibble::tibble(
Specificity = spec,
Sensitivity = rec,
Accuracy    = acc,
`F1 score`  = f1
) |>
dplyr::mutate(dplyr::across(dplyr::everything(), ~ base::sprintf(base::paste0("%.", digits, "f"), .x)))
tbl <- gridExtra::tableGrob(
df_wide, rows = NULL,
theme = gridExtra::ttheme_minimal(
core   = base::list(fg_params = base::list(fontface = 1, just = "center")),
colhead= base::list(fg_params = base::list(fontface = 2), bg_params = base::list(fill = "#e6e6e6", col = NA))
)
)
# Colonnes largeur égale
tbl$widths <- grid::unit(rep(1 / base::ncol(df_wide), base::ncol(df_wide)), "npc")
p <- ggplotify::as.ggplot(tbl)
if (isTRUE(verbose)) base::message("[cm] tableau métriques prêt.")
return(p)
}
# ============================================================================
#  NOUVELLES FONCTIONS (consolidation) — uniquement 2 entrées publiques
# ============================================================================
#' Panneau complet : matrice de confusion + tableau des métriques (à partir des prédictions)
#'
#' @param y_true factor|character|integer Vérités terrain de longueur n
#' @param y_pred factor|character|integer Prédictions de longueur n (mêmes labels que y_true si factor)
#' @param labels character(NULL) Niveaux/ordre des classes (facultatif). Si NULL, union triée des niveaux/valeurs observées.
#' @param title character Titre du panneau (heatmap en haut, tableau métriques en bas)
#' @param digits integer Décimales pour le tableau des métriques
#' @param show_bottomright logical Afficher la somme globale en (sum,sum)
#' @param verbose logical Messages (FALSE)
#' @return patchwork Plot patchwork (via patchwork::wrap_plots)
#' @details
#' - Calcule en interne : matrice de confusion (Truth x Pred), métriques par classe (TP/FP/FN/TN,
#'   precision/recall/specificity/F1), macro-agrégats, puis crée le data.frame long avec ligne/colonne "sum".
#' - Utilise ensuite `viz_confusion_heatmap_sums()` (conservée) et `metrics_table_plot()` (conservée).
#' - Entrées acceptées : vecteurs factor/character/integer ; les facteurs sont réalignés
#'   sur \code{labels} si fournis.
viz_confusion_panel_from_predictions <- function(y_true,
y_pred,
labels = NULL,
title = "Confusion Matrix (Test)",
digits = 2L,
show_bottomright = FALSE,
verbose = FALSE) {
# -- Contrôles I/O --
stopifnot(length(y_true) == length(y_pred))
n <- base::length(y_true)
if (isTRUE(verbose)) base::message("[cm-panel] n=", n)
# -- Harmonisation des labels/levels --
if (base::is.null(labels)) {
labs <- base::sort(base::unique(base::c(base::as.character(y_true), base::as.character(y_pred))))
} else {
labs <- base::as.character(labels)
}
y_true_f <- base::factor(base::as.character(y_true), levels = labs)
y_pred_f <- base::factor(base::as.character(y_pred), levels = labs)
# -- Matrice de confusion Truth x Pred (avec dimnames) --
cm_tab <- base::table(Truth = y_true_f, Pred = y_pred_f)
cm <- base::as.matrix(cm_tab)
# -- Métriques (par classe & macro) --
classes <- base::rownames(cm)
total <- base::sum(cm)
acc <- if (total > 0) base::sum(base::diag(cm)) / total else NA_real_
per <- lapply(base::seq_along(classes), function(i) {
TP <- cm[i, i]
FP <- base::sum(cm[, i]) - TP
FN <- base::sum(cm[i, ]) - TP
TN <- total - TP - FP - FN
precision   <- base::ifelse((TP + FP) == 0, NA_real_, TP / (TP + FP))
recall      <- base::ifelse((TP + FN) == 0, NA_real_, TP / (TP + FN))
specificity <- base::ifelse((TN + FP) == 0, NA_real_, TN / (TN + FP))
denom <- precision + recall
f1 <- if (base::is.na(precision) || base::is.na(recall) || base::isTRUE(denom == 0)) NA_real_ else 2 * precision * recall / denom
tibble::tibble(class = classes[i], TP = TP, FP = FP, FN = FN, TN = TN,
precision = precision, recall = recall, specificity = specificity, f1 = f1)
}) |> dplyr::bind_rows()
macro_tbl <- tibble::tibble(
macro_precision = base::mean(per$precision, na.rm = TRUE),
macro_recall    = base::mean(per$recall,    na.rm = TRUE),
macro_f1        = base::mean(per$f1,        na.rm = TRUE),
weighted_f1     = stats::weighted.mean(per$f1, w = per$TP + per$FN, na.rm = TRUE)
)
metrics_list <- base::list(accuracy = acc, per_class_tbl = per, macro_tbl = macro_tbl)
if (isTRUE(verbose)) base::message(base::sprintf("[cm-panel] acc=%.4f, macro_f1=%.4f", acc, macro_tbl$macro_f1))
# -- df long avec ligne/colonne "sum" (interne) --
classes <- base::colnames(cm)
x_levels <- base::c(classes, "sum"); y_levels <- base::c(classes, "sum")
core <- base::as.data.frame(stats::xtabs(Freq ~ Truth + Pred,
data = stats::setNames(base::as.data.frame(base::as.table(cm)), c("Truth","Pred","Freq"))),
stringsAsFactors = FALSE)
row_sum <- stats::aggregate(Freq ~ Truth, core, sum)
row_sum$Pred <- "sum"; row_sum <- row_sum[, c("Truth","Pred","Freq")]
col_sum <- stats::aggregate(Freq ~ Pred, core, sum)
col_sum$Truth <- "sum"; col_sum <- col_sum[, c("Truth","Pred","Freq")]
corner <- base::data.frame(Truth = "sum", Pred = "sum",
Freq = if (isTRUE(show_bottomright)) base::sum(cm) else NA_real_)
df_long <- dplyr::bind_rows(core, row_sum, col_sum, corner)
df_long <- tidyr::complete(df_long, Truth = y_levels, Pred = x_levels, fill = base::list(Freq = NA_real_))
df_long$Truth <- base::factor(df_long$Truth, levels = y_levels)
df_long$Pred  <- base::factor(df_long$Pred,  levels = x_levels)
df_long$is_sum <- (df_long$Truth == "sum") | (df_long$Pred == "sum")
if (isTRUE(verbose)) base::message("[cm-panel] df_long: ", nrow(df_long), " lignes")
# -- Figures (heatmap + tableau) --
p_cm  <- viz_confusion_heatmap_sums(df_long, title = title, verbose = verbose)
p_tbl <- metrics_table_plot(metrics_list, digits = digits, verbose = verbose)
p <- patchwork::wrap_plots(p_cm, p_tbl, ncol = 1, heights = c(3, 1))
return(p)
}
#' Courbes d'entraînement (history Keras) : loss & accuracy (train/val)
#'
#' @param history list|object Objet renvoyé par keras3::fit(...) (R) ou Python (reticulate), contenant les clés de métriques
#' @param title character Titre du graphique
#' @param metrics character Vecteur des métriques cibles à afficher (par défaut c("loss","accuracy"))
#' @param smooth logical Appliquer un lissage LOESS (facultatif, par défaut FALSE)
#' @param verbose logical Messages (FALSE)
#' @return ggplot2::ggplot Un graphique facetté par métrique, couleurs distinctes pour train/val
#' @details
#' - Conversion robuste de l'history : supporte objets python via reticulate::py_to_r(history$history).
#' - Alias automatiques : \code{sparse_categorical_accuracy} -> \code{accuracy} (idem pour validation).
#' - Séries tracées : train vs validation (si présentes) pour chaque métrique demandée ; facettes \code{loss} et \code{accuracy}.
history_curves_plot <- function(history,
title = "Training curves",
metrics = c("loss", "accuracy"),
smooth = FALSE,
verbose = FALSE) {
df <- NULL
# Tentative reticulate si disponible et structure python
if (base::requireNamespace("reticulate", quietly = TRUE)) {
h <- base::try(reticulate::py_to_r(history$history), silent = TRUE)
if (!inherits(h, "try-error") && base::is.list(h) && length(h) > 0) {
df <- base::as.data.frame(h, optional = TRUE, check.names = FALSE)
n  <- if (!base::is.null(df$loss)) base::length(df$loss) else base::max(base::lengths(h))
df$epoch <- base::seq_len(n)
}
}
# Fallback générique
if (base::is.null(df)) df <- base::try(base::as.data.frame(history), silent = TRUE)
if (inherits(df, "try-error") || base::is.null(df) || base::ncol(df) == 0) base::stop("Aucune métrique trouvée dans 'history'.")
if (!("epoch" %in% base::tolower(base::names(df)))) df$epoch <- base::seq_len(base::nrow(df))
# Alias utiles
if ("sparse_categorical_accuracy" %in% base::names(df) && !("accuracy" %in% base::names(df))) df$accuracy <- df$sparse_categorical_accuracy
if ("val_sparse_categorical_accuracy" %in% base::names(df) && !("val_accuracy" %in% base::names(df))) df$val_accuracy <- df$val_sparse_categorical_accuracy
# Colonnes candidates
keep <- base::intersect(base::c("loss", "val_loss", "accuracy", "val_accuracy"), base::names(df))
if (length(keep) == 0) base::stop("Aucune des colonnes attendues ('loss', 'val_loss', 'accuracy', 'val_accuracy') n'a été trouvée.")
# Filtrer par métriques demandées
want <- base::unique(base::tolower(metrics))
cols <- base::c("epoch")
if ("loss" %in% want) cols <- base::c(cols, base::intersect(base::c("loss", "val_loss"), keep))
if ("accuracy" %in% want) cols <- base::c(cols, base::intersect(base::c("accuracy", "val_accuracy"), keep))
cols <- base::unique(cols)
df2 <- df[, cols, drop = FALSE]
# Long format + étiquettes propres
long <- tidyr::pivot_longer(df2, cols = -"epoch", names_to = "series", values_to = "value") |>
tidyr::drop_na(value)
long$metric <- dplyr::case_when(
long$series %in% c("loss", "val_loss") ~ "loss",
long$series %in% c("accuracy", "val_accuracy") ~ "accuracy",
TRUE ~ long$series
)
long$split <- dplyr::if_else(base::grepl("^val", long$series), "validation", "train")
long$series_clean <- base::paste0(long$metric, " (", long$split, ")")
# Palette manuelle (sobre & contrastée)
pal <- c(
"loss (train)"          = "#1f77b4",
"loss (validation)"     = "#6baed6",
"accuracy (train)"      = "#2ca02c",
"accuracy (validation)" = "#74c476"
)
pal <- pal[names(pal) %in% base::unique(long$series_clean)]
p <- ggplot2::ggplot(long, ggplot2::aes(x = .data$epoch, y = .data$value, color = .data$series_clean)) +
ggplot2::geom_line(linewidth = 0.9, alpha = 0.95)
if (isTRUE(smooth)) {
p <- p + ggplot2::geom_smooth(se = FALSE, method = "loess", span = 0.25, linewidth = 0.6, alpha = 0.6)
}
p <- p +
ggplot2::scale_color_manual(values = pal) +
ggplot2::facet_wrap(~ metric, scales = "free_y", ncol = 1, labeller = ggplot2::label_both) +
ggplot2::labs(title = title, x = "Epoch", y = "Valeur", color = "Séries") +
ggplot2::theme_minimal(base_size = 13) +
ggplot2::theme(legend.position = "top",
strip.text = ggplot2::element_text(face = "bold"))
return(p)
}
source("R/metrics.R")
# Demo confusion panel
set.seed(1)
y_true <- factor(sample(letters[1:3], 60, TRUE), levels = letters[1:3])
y_pred <- factor(sample(letters[1:3], 60, TRUE), levels = letters[1:3])
print(viz_confusion_panel_from_predictions(y_true, y_pred, title = "Demo — CM"))
# Demo curves
history_like <- as.data.frame(list(
loss = seq(1, 0.4, length.out = 20),
val_loss = seq(1.2, 0.5, length.out = 20),
accuracy = seq(0.4, 0.9, length.out = 20),
val_accuracy = seq(0.35, 0.85, length.out = 20)
))
print(history_curves_plot(history_like, title = "Demo — Training curves"))
source("run_iris.R"); run_iris_demo(seed = 123, verbose = TRUE)
source("inst/examples/run_iris.R"); run_iris_demo(seed = 123, verbose = TRUE)
View(viz_confusion_heatmap_sums)
